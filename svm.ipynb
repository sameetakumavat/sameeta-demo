{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1bfaa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python pillow scikit-image pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75c03d6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "import yaml\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - UPDATE THESE PATHS FOR YOUR SETUP\n",
    "# ============================================================================\n",
    "IMG_WIDTH = 800\n",
    "IMG_HEIGHT = 600\n",
    "GRID_ROWS = 8\n",
    "GRID_COLS = 8\n",
    "\n",
    "# Path to labels.csv (contains ImageFileName, TrainOrTest, c01-c64 columns)\n",
    "LABELS_CSV = r'C:\\Users\\sakumavat\\Downloads\\test\\labels.csv'\n",
    "\n",
    "# Directory containing your actual images (must match ImageFileName in labels.csv)\n",
    "RAW_IMAGES_DIR = r'C:\\Users\\sakumavat\\Downloads\\test\\processed_images'\n",
    "\n",
    "# Output directory for extracted features\n",
    "EXPERIMENTS_DIR = r'C:\\Users\\sakumavat\\Downloads\\test\\results'\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_labels(csv_path):\n",
    "    \"\"\"Loads labels from CSV and filters for 'Train' images.\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Error: {csv_path} not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if 'TrainOrTest' in df.columns:\n",
    "        df = df[df['TrainOrTest'] == 'Train']\n",
    "    return df\n",
    "\n",
    "def get_cell_coordinates(cell_idx, grid_rows=GRID_ROWS, grid_cols=GRID_COLS, img_w=IMG_WIDTH, img_h=IMG_HEIGHT):\n",
    "    \"\"\"Returns (x1, y1, x2, y2) for a 0-based cell index.\"\"\"\n",
    "    cell_w = img_w / grid_cols\n",
    "    cell_h = img_h / grid_rows\n",
    "    row = cell_idx // grid_cols\n",
    "    col = cell_idx % grid_cols\n",
    "    x1, y1 = int(col * cell_w), int(row * cell_h)\n",
    "    x2, y2 = int(x1 + cell_w), int(y1 + cell_h)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def extract_hog_features(cell_gray, params):\n",
    "    \"\"\"Extracts HOG features from a grayscale cell.\"\"\"\n",
    "    fd = hog(cell_gray,\n",
    "             orientations=params['orientations'],\n",
    "             pixels_per_cell=params['pixels_per_cell'],\n",
    "             cells_per_block=params['cells_per_block'],\n",
    "             visualize=False,\n",
    "             channel_axis=None)\n",
    "    return fd\n",
    "\n",
    "def extract_lbp_features(cell_gray, params):\n",
    "    \"\"\"Extracts LBP histogram from a grayscale cell.\"\"\"\n",
    "    radius = params['radius']\n",
    "    n_points = params['n_points']\n",
    "    method = params['method']\n",
    "    lbp = local_binary_pattern(cell_gray, n_points, radius, method)\n",
    "    n_bins = int(n_points + 2)\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "def extract_color_histogram(cell_rgb, params):\n",
    "    \"\"\"Extracts Color Histogram from an RGB cell.\"\"\"\n",
    "    bins = params['bins']\n",
    "    hist_r, _ = np.histogram(cell_rgb[:, :, 0], bins=bins, range=(0, 256), density=True)\n",
    "    hist_g, _ = np.histogram(cell_rgb[:, :, 1], bins=bins, range=(0, 256), density=True)\n",
    "    hist_b, _ = np.histogram(cell_rgb[:, :, 2], bins=bins, range=(0, 256), density=True)\n",
    "    return np.concatenate([hist_r, hist_g, hist_b])\n",
    "\n",
    "def save_iteration_metadata(output_dir, params, metadata):\n",
    "    \"\"\"Saves parameters and metadata to a YAML file.\"\"\"\n",
    "    filepath = os.path.join(output_dir, \"parameters.yml\")\n",
    "    label_names = {0: \"None\", 1: \"Ball\", 2: \"Bat\", 3: \"Stump\"}\n",
    "    readable_label_counts = {label_names.get(label, f\"Unknown({label})\"): count \n",
    "                            for label, count in metadata['label_counts'].items()}\n",
    "    \n",
    "    data = {\n",
    "        \"iteration_name\": metadata['name'],\n",
    "        \"description\": metadata['description'],\n",
    "        \"parameters\": params,\n",
    "        \"metadata\": {\n",
    "            \"total_images\": metadata['total_images'],\n",
    "            \"total_cells\": metadata['total_cells'],\n",
    "            \"label_counts\": readable_label_counts,\n",
    "            \"feature_dimensions\": metadata['feature_dims']\n",
    "        }\n",
    "    }\n",
    "    with open(filepath, \"w\") as f:\n",
    "        yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def run_extraction(iteration_name, iteration_desc, params):\n",
    "    \"\"\"Main feature extraction function.\"\"\"\n",
    "    # Setup output directory\n",
    "    output_dir = os.path.join(EXPERIMENTS_DIR, iteration_name)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(f\"--- Starting Feature Extraction: {iteration_name} ---\")\n",
    "    print(f\"Output Directory: {output_dir}\\n\")\n",
    "\n",
    "    # Load labels\n",
    "    df_labels = load_labels(LABELS_CSV)\n",
    "    if df_labels.empty:\n",
    "        print(\"No labels found or empty file.\")\n",
    "        return None\n",
    "\n",
    "    total_images = len(df_labels)\n",
    "    print(f\"Found {total_images} images in labels.csv\")\n",
    "    \n",
    "    # Validate images exist\n",
    "    print(\"Validating image files...\")\n",
    "    missing_images = []\n",
    "    existing_images = []\n",
    "    \n",
    "    for idx, row in df_labels.iterrows():\n",
    "        filename = row['ImageFileName']\n",
    "        filepath = os.path.join(RAW_IMAGES_DIR, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            missing_images.append(filename)\n",
    "        else:\n",
    "            existing_images.append(filename)\n",
    "    \n",
    "    if missing_images:\n",
    "        print(f\"⚠️  WARNING: {len(missing_images)} images not found (will be skipped)\")\n",
    "        print(f\"✓  Will process {len(existing_images)} available images\\n\")\n",
    "    else:\n",
    "        print(f\"✓  All {total_images} images found!\\n\")\n",
    "\n",
    "    # Extract features\n",
    "    all_features = []\n",
    "    label_counts = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "    feature_dims = {}\n",
    "    processed_count = 0\n",
    "\n",
    "    for idx, row in df_labels.iterrows():\n",
    "        filename = row['ImageFileName']\n",
    "        filepath = os.path.join(RAW_IMAGES_DIR, filename)\n",
    "\n",
    "        if not os.path.exists(filepath):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {processed_count+1}/{len(existing_images)}: {filename}\")\n",
    "        processed_count += 1\n",
    "\n",
    "        try:\n",
    "            # Load and resize image\n",
    "            pil_img = Image.open(filepath)\n",
    "            pil_img = pil_img.resize((IMG_WIDTH, IMG_HEIGHT), Image.Resampling.LANCZOS)\n",
    "            img_rgb = np.array(pil_img.convert(\"RGB\"))\n",
    "            img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "            # Process each cell in 8x8 grid\n",
    "            for i in range(GRID_ROWS * GRID_COLS):\n",
    "                # Get label for this cell\n",
    "                label_col = f\"c{i+1:02d}\"\n",
    "                if label_col not in row:\n",
    "                    continue\n",
    "                label = int(row[label_col])\n",
    "                label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "                # Extract cell image\n",
    "                x1, y1, x2, y2 = get_cell_coordinates(i)\n",
    "                cell_rgb = img_rgb[y1:y2, x1:x2]\n",
    "                cell_gray = img_gray[y1:y2, x1:x2]\n",
    "\n",
    "                # Extract features\n",
    "                hog_vec = extract_hog_features(cell_gray, params['hog'])\n",
    "                lbp_vec = extract_lbp_features(cell_gray, params['lbp'])\n",
    "                color_vec = extract_color_histogram(cell_rgb, params['color'])\n",
    "\n",
    "                # Store feature dimensions (first cell only)\n",
    "                if not feature_dims:\n",
    "                    feature_dims = {\n",
    "                        'hog': len(hog_vec),\n",
    "                        'lbp': len(lbp_vec),\n",
    "                        'color': len(color_vec),\n",
    "                        'total': len(hog_vec) + len(lbp_vec) + len(color_vec)\n",
    "                    }\n",
    "\n",
    "                # Assemble feature row\n",
    "                feature_row = {\n",
    "                    \"image_file_name\": filename,  # Links to original image\n",
    "                    \"cell_number\": i + 1,\n",
    "                    \"row_idx\": i // GRID_COLS,\n",
    "                    \"col_idx\": i % GRID_COLS,\n",
    "                    \"label\": label\n",
    "                }\n",
    "\n",
    "                # Add all feature values\n",
    "                for j, val in enumerate(hog_vec):\n",
    "                    feature_row[f\"hog_{j}\"] = val\n",
    "                for j, val in enumerate(lbp_vec):\n",
    "                    feature_row[f\"lbp_{j}\"] = val\n",
    "                for j, val in enumerate(color_vec):\n",
    "                    feature_row[f\"color_{j}\"] = val\n",
    "\n",
    "                all_features.append(feature_row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing {filename}: {e}\")\n",
    "\n",
    "    # Save results\n",
    "    if not all_features:\n",
    "        print(\"\\nERROR: No features extracted! Check if images exist and are readable.\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"\\n✓ Successfully extracted features from {processed_count} images ({len(all_features)} cells)\")\n",
    "    print(\"Saving features.csv...\")\n",
    "    \n",
    "    df_features = pd.DataFrame(all_features)\n",
    "    csv_path = os.path.join(output_dir, \"features.csv\")\n",
    "    df_features.to_csv(csv_path, index=False)\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"name\": iteration_name,\n",
    "        \"description\": iteration_desc,\n",
    "        \"total_images\": processed_count,\n",
    "        \"total_cells\": len(all_features),\n",
    "        \"label_counts\": label_counts,\n",
    "        \"feature_dims\": feature_dims\n",
    "    }\n",
    "    save_iteration_metadata(output_dir, params, metadata)\n",
    "\n",
    "    print(f\"✓ Features saved to: {csv_path}\")\n",
    "    print(\"✓ Done!\\n\")\n",
    "    return csv_path\n",
    "\n",
    "# ============================================================================\n",
    "# RUN FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "ITERATION_NAME = \"final_iter_03_baseline\"\n",
    "ITERATION_DESC = \"Baseline feature extraction with HOG, LBP, and Color Histogram\"\n",
    "\n",
    "PARAMS = {\n",
    "    'hog': {\n",
    "        'orientations': 9,\n",
    "        'pixels_per_cell': (8, 8),\n",
    "        'cells_per_block': (2, 2)\n",
    "    },\n",
    "    'lbp': {\n",
    "        'radius': 1,\n",
    "        'n_points': 8,\n",
    "        'method': 'uniform'\n",
    "    },\n",
    "    'color': {\n",
    "        'bins': 32\n",
    "    }\n",
    "}\n",
    "\n",
    "features_csv_path = run_extraction(ITERATION_NAME, ITERATION_DESC, PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bcd3c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\sakumavat\\appdata\\local\\anaconda3\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\sakumavat\\appdata\\local\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\sakumavat\\appdata\\roaming\\python\\python313\\site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sakumavat\\appdata\\local\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\sakumavat\\appdata\\local\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sakumavat\\appdata\\roaming\\python\\python313\\site-packages (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\sakumavat\\appdata\\roaming\\python\\python313\\site-packages (from xgboost) (1.16.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sakumavat\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sakumavat\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sakumavat\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sakumavat\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sakumavat\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sakumavat\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sakumavat\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sakumavat\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\sakumavat\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sakumavat\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sakumavat\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sakumavat\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sakumavat\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042525bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training: SVM (final_iter_01_SVM) ---\n",
      "Loading data...\n",
      "Using full dataset for training...\n",
      "Scaling features...\n",
      "Training SVM...\n",
      "Evaluating on Train Data...\n",
      "Evaluating on Train Data...\n",
      "Train Accuracy: 0.3855\n",
      "Train ROC AUC: 0.6919\n",
      "Train Accuracy: 0.3855\n",
      "Train ROC AUC: 0.6919\n",
      "Artifacts saved to C:\\Users\\sakumavat\\Downloads\\test\\experiments\\training\\final_iter_01_SVM\n",
      "Artifacts saved to C:\\Users\\sakumavat\\Downloads\\test\\experiments\\training\\final_iter_01_SVM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - UPDATE THESE PATHS FOR YOUR SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Path to features.csv (generated by feature extraction cell above)\n",
    "FEATURES_CSV_PATH = r\"C:\\Users\\sakumavat\\Downloads\\test\\results\\final_iter_03_baseline\\features.csv\"\n",
    "\n",
    "# Output directory for trained model and results\n",
    "OUTPUT_BASE_DIR = r\"C:\\Users\\sakumavat\\Downloads\\test\\experiments\\training\"\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL DEFINITIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_model(model_name):\n",
    "    \"\"\"Returns the specified model.\"\"\"\n",
    "    if model_name == \"SVM\":\n",
    "        return SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=RANDOM_STATE)\n",
    "    elif model_name == \"RF\":\n",
    "        return RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=-1, random_state=RANDOM_STATE)\n",
    "    elif model_name == \"XGB\":\n",
    "        return xgb.XGBClassifier(objective='multi:softprob', eval_metric='mlogloss', random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_data(csv_path):\n",
    "    \"\"\"Loads features and separates X and y from the extracted features CSV.\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"{csv_path} not found. Run feature extraction first!\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Metadata columns (not used for training)\n",
    "    drop_cols = ['image_file_name', 'cell_number', 'row_idx', 'col_idx', 'label']\n",
    "    \n",
    "    # X = actual image features (HOG, LBP, Color)\n",
    "    X = df.drop(columns=drop_cols)\n",
    "    # y = labels (0=None, 1=Ball, 2=Bat, 3=Stump)\n",
    "    y = df['label']\n",
    "    \n",
    "    metadata = df[drop_cols]\n",
    "    return X, y, metadata\n",
    "\n",
    "def plot_roc_auc(y_test, y_score, classes, output_path):\n",
    "    \"\"\"Plots ROC AUC for multiclass.\"\"\"\n",
    "    y_test_bin = label_binarize(y_test, classes=classes)\n",
    "    n_classes = y_test_bin.shape[1]\n",
    "\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        roc_auc[i] = roc_auc_score(y_test_bin[:, i], y_score[:, i])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    label_names = {0: \"None\", 1: \"Ball\", 2: \"Bat\", 3: \"Stump\"}\n",
    "\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        label_name = label_names.get(classes[i], str(classes[i]))\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'ROC curve of class {label_name} (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) - Multiclass')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def save_artifacts(output_dir, model, scaler, metrics_df, predictions_df):\n",
    "    \"\"\"Saves model, scaler, metrics, and predictions.\"\"\"\n",
    "    # Save model as pickle (for later use/deployment)\n",
    "    with open(os.path.join(output_dir, \"model.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # Save scaler as pickle\n",
    "    if scaler:\n",
    "        with open(os.path.join(output_dir, \"scaler.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(scaler, f)\n",
    "\n",
    "    # Save metrics and predictions as CSV\n",
    "    metrics_df.to_csv(os.path.join(output_dir, \"metrics.csv\"), index=False)\n",
    "    predictions_df.to_csv(os.path.join(output_dir, \"predicted_labels.csv\"), index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN TRAINING FUNCTION (USES 100% OF DATA)\n",
    "# ============================================================================\n",
    "\n",
    "def train_and_evaluate(model_name, iteration_name):\n",
    "    \"\"\"Main training loop - uses 100% of data for production model.\"\"\"\n",
    "    # Setup output directory\n",
    "    output_dir = os.path.join(OUTPUT_BASE_DIR, iteration_name)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Training: {model_name} ({iteration_name})\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Load features\n",
    "    print(\"Loading features...\")\n",
    "    X, y, metadata = load_data(FEATURES_CSV_PATH)\n",
    "    \n",
    "    print(f\"✓ Features shape: {X.shape}\")\n",
    "    print(f\"✓ Labels shape: {y.shape}\")\n",
    "    print(f\"Label distribution:\")\n",
    "    print(y.value_counts().sort_index())\n",
    "    print()\n",
    "\n",
    "    # Use full dataset (no split for production)\n",
    "    print(\"Using 100% of data for training (production model)...\")\n",
    "    X_train, y_train = X, y\n",
    "\n",
    "    # Scale features\n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Train model\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model = get_model(model_name)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Evaluate on training data\n",
    "    print(\"\\nEvaluating on training data...\")\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_train_prob = model.predict_proba(X_train_scaled)\n",
    "\n",
    "    train_report = classification_report(y_train, y_train_pred, output_dict=True)\n",
    "    train_auc = roc_auc_score(label_binarize(y_train, classes=model.classes_), y_train_prob, multi_class='ovr')\n",
    "\n",
    "    print(f\"✓ Train Accuracy: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"✓ Train ROC AUC: {train_auc:.4f}\")\n",
    "\n",
    "    # Generate metrics\n",
    "    metrics_list = []\n",
    "    for label, metrics in train_report.items():\n",
    "        if isinstance(metrics, dict):\n",
    "            metrics_list.append({\n",
    "                \"Dataset\": \"Train\",\n",
    "                \"Class\": label,\n",
    "                \"Precision\": metrics['precision'],\n",
    "                \"Recall\": metrics['recall'],\n",
    "                \"F1-Score\": metrics['f1-score'],\n",
    "                \"Support\": metrics['support']\n",
    "            })\n",
    "    metrics_list.append({\n",
    "        \"Dataset\": \"Train\", \n",
    "        \"Class\": \"Overall\", \n",
    "        \"ROC_AUC\": train_auc, \n",
    "        \"Accuracy\": accuracy_score(y_train, y_train_pred)\n",
    "    })\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "    # Generate predictions\n",
    "    full_metadata = metadata.copy()\n",
    "    full_metadata['True_Label'] = y\n",
    "    full_metadata['Predicted_Label'] = y_train_pred\n",
    "    full_metadata[['Prob_0', 'Prob_1', 'Prob_2', 'Prob_3']] = y_train_prob\n",
    "    full_metadata['Split'] = 'Train'\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plot_roc_auc(y_train, y_train_prob, model.classes_, os.path.join(output_dir, \"roc_auc_train.png\"))\n",
    "\n",
    "    # Reshape to wide format (matching labels.csv format)\n",
    "    wide_df = full_metadata.pivot(index='image_file_name', columns='cell_number', values='Predicted_Label')\n",
    "    wide_df.columns = [f\"c{col:02d}\" for col in wide_df.columns]\n",
    "    wide_df = wide_df.reset_index()\n",
    "    wide_df.rename(columns={'image_file_name': 'ImageFileName'}, inplace=True)\n",
    "    wide_df['TrainOrTest'] = 'Train'\n",
    "\n",
    "    # Reorder columns to match labels.csv\n",
    "    cols = [\"ImageFileName\", \"TrainOrTest\"] + [f\"c{i+1:02d}\" for i in range(64)]\n",
    "    for col in cols:\n",
    "        if col not in wide_df.columns:\n",
    "            wide_df[col] = 0\n",
    "    wide_df = wide_df[cols]\n",
    "\n",
    "    # Save everything\n",
    "    save_artifacts(output_dir, model, scaler, metrics_df, wide_df)\n",
    "\n",
    "    print(f\"\\n✓ Model saved to: {os.path.join(output_dir, 'model.pkl')}\")\n",
    "    print(f\"✓ Scaler saved to: {os.path.join(output_dir, 'scaler.pkl')}\")\n",
    "    print(f\"✓ All artifacts saved to: {output_dir}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "# Select model: \"SVM\", \"RF\", or \"XGB\"\n",
    "MODEL_TO_TRAIN = \"SVM\"\n",
    "ITERATION_NAME = \"final_iter_01_SVM\"\n",
    "\n",
    "train_and_evaluate(MODEL_TO_TRAIN, ITERATION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b799b5",
   "metadata": {},
   "source": [
    "# Optional: Test Model with Train/Test Split\n",
    "\n",
    "**NOTE:** This cell is for testing only! It splits data 80/20 to evaluate model performance.\n",
    "\n",
    "The actual production model (Cell 4 above) uses 100% of data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a93d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEST EVALUATION WITH TRAIN/TEST SPLIT (FOR TESTING ONLY)\n",
    "# ============================================================================\n",
    "\n",
    "def test_model_with_split(model_name, test_size=0.2):\n",
    "    \"\"\"Test model performance with train/test split.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEST MODE: Evaluating {model_name} with {test_size*100:.0f}% test split\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading features...\")\n",
    "    X, y, metadata = load_data(FEATURES_CSV_PATH)\n",
    "    \n",
    "    print(f\"Total samples: {len(X)}\")\n",
    "    print(f\"Features shape: {X.shape}\\n\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Testing samples: {len(X_test)}\\n\")\n",
    "    \n",
    "    # Scale features\n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Training {model_name}...\\n\")\n",
    "    model = get_model(model_name)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate on train data\n",
    "    print(\"--- Train Set Performance ---\")\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_train_prob = model.predict_proba(X_train_scaled)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    train_auc = roc_auc_score(\n",
    "        label_binarize(y_train, classes=model.classes_), \n",
    "        y_train_prob, \n",
    "        multi_class='ovr'\n",
    "    )\n",
    "    \n",
    "    print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Train ROC AUC: {train_auc:.4f}\")\n",
    "    print(\"\\nTrain Classification Report:\")\n",
    "    print(classification_report(y_train, y_train_pred, target_names=[\"None\", \"Ball\", \"Bat\", \"Stump\"]))\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    print(\"\\n--- Test Set Performance ---\")\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    y_test_prob = model.predict_proba(X_test_scaled)\n",
    "    \n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_auc = roc_auc_score(\n",
    "        label_binarize(y_test, classes=model.classes_), \n",
    "        y_test_prob, \n",
    "        multi_class='ovr'\n",
    "    )\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test ROC AUC: {test_auc:.4f}\")\n",
    "    print(\"\\nTest Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=[\"None\", \"Ball\", \"Bat\", \"Stump\"]))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix (Test Set):\")\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=[\"None\", \"Ball\", \"Bat\", \"Stump\"],\n",
    "                yticklabels=[\"None\", \"Ball\", \"Bat\", \"Stump\"])\n",
    "    plt.title(f'{model_name} - Confusion Matrix (Test Set)')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY:\")\n",
    "    print(f\"  Train Accuracy: {train_acc:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Train ROC AUC:  {train_auc:.4f} | Test ROC AUC:  {test_auc:.4f}\")\n",
    "    \n",
    "    overfitting = (train_acc - test_acc) > 0.1\n",
    "    print(f\"  Overfitting: {'⚠️  Yes (difference > 10%)' if overfitting else '✓ No (good generalization)'}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'train_auc': train_auc,\n",
    "        'test_auc': test_auc\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# RUN TEST EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "# Select model to test: \"SVM\", \"RF\", or \"XGB\"\n",
    "TEST_MODEL = \"SVM\"\n",
    "results = test_model_with_split(TEST_MODEL, test_size=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
